# MAPLE: Model-Adaptive Projection of Latent Embeddings
**Final Research Report**
*Date: 2026-02-12*

## 1. Abstract
We present MAPLE, a neural scanner designed for high-efficiency information retrieval over long contexts. 
Evaluation on **NarrativeQA** (Fiction) and **HotpotQA** (Fact) demonstrates that our "Generalist" model achieves **98.0% Recall@1** on narrative queries and **60.2% Recall@5** on multi-hop factual queries (a 3x improvement over zero-shot baseline).
However, we identify a significant limitation in **Robustness**: the model exhibits a "Narrative Bias" that suppresses out-of-distribution facts in fiction, resulting in a **0.0%** success rate on the Needle-in-a-Haystack benchmark.

## 2. Efficiency
MAPLE utilizes a hierarchical `Adaptive` search strategy that scales **Sub-Linearly** with respect to index size, enabling distinct speedups over Linear scanning at scale.

| Index Size (Blocks) | Linear Scan Time | Adaptive Scan Time | Speedup |
|---------------------|------------------|--------------------|---------|
| 5,000,000 (100 Books) | ~1200s (Est.) | 8.63s | **>100x** |

*See `paper_assets/Fig2_Efficiency.png` for scaling curves.*

## 3. Domain Generalization
Our Generalist model was trained on a mixed corpus of 500 NarrativeQA samples and only 10 HotpotQA samples.

| Domain | Metric | Baseline (Zero-Shot) | Generalist (Few-Shot) | Improvement |
|--------|--------|----------------------|-----------------------|-------------|
| NarrativeQA | Recall@1 | 98.0% | **98.0%** | Maintained |
| HotpotQA | Recall@5 | 24.5% | **60.2%** | **+35.7%** |

*See `paper_assets/Fig1_Recall.png` for performance distribution.*

## 4. Limitations & Robustness
Usage of MAPLE restricts fine-grained fact retrieval within dense narrative text.
- **Phenomenon:** "Needle Suppression"
- **Observation:** The MLP assigns low relevance scores (~0.29) to factual sentences inserted into fiction, while assigning high relevance (>0.90) to narrative flow and metadata headers.
- **Benchmark:** Needle-in-a-Haystack (Alice in Wonderland).
- **Result:** **0.0% Success**.
- **Root Cause:** Deep embedding bias in the frozen BGE encoder towards "Story" semantics, which the lightweight MLP cannot easily override without overfitting.

## 5. Hardware Statistics
- **Model Size:** ~6MB (MLP weights) + frozen BGE-Small.
- **Peak VRAM:** ~4GB (Training), ~2GB (Inference).
- **Training Time:** ~5 minutes (Generalist Experiment, RTX 4090 equivalent).

---
*Generated by Scout-KV Research Team*
