#!/usr/bin/env python3
"""
MAPLE Model Trainer
====================
Train MapleNet from oracle attention labels generated by
``scripts/generate_oracle.py``.

Usage:
    python scripts/train_maple.py                     # default settings
    python scripts/train_maple.py --epochs 30 --lr 5e-4
"""

from __future__ import annotations

import argparse
import json
import logging
import random
import sys
import time
from pathlib import Path
from typing import Dict, List

import matplotlib
matplotlib.use("Agg")            # headless backend for server / CI
import matplotlib.pyplot as plt
import torch

# ---------------------------------------------------------------------------
# Project imports
# ---------------------------------------------------------------------------
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from maplecore import MapleIndexer, MapleTrainer

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
ORACLE_PATH = Path("data/oracle_data.json")
MODEL_DIR = Path("models")
MODEL_PATH = MODEL_DIR / "maple_v1.pth"
CURVE_PATH = Path("benchmarks/results/training_curve.png")


# ===================================================================
# Data Preparation
# ===================================================================

def load_oracle(path: Path) -> dict:
    """Load oracle data produced by generate_oracle.py."""
    logger.info(f"Loading oracle data from {path}")
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    logger.info(
        f"Loaded {len(data['samples'])} samples "
        f"(model: {data['metadata']['model']})"
    )
    return data


def build_training_data(
    oracle: dict,
    indexer: MapleIndexer,
    neg_ratio: int = 1,
) -> List[Dict]:
    """
    Convert oracle samples into (query_emb, block_emb, label) triples.

    For each oracle sample:
      - Positive: top-5 blocks (label=1)
      - Negative: randomly sampled non-top-5 blocks (label=0)
        with ``neg_ratio`` negatives per positive.

    Args:
        oracle: Oracle data dict from generate_oracle.py
        indexer: MapleIndexer with BGE model loaded
        neg_ratio: Number of negative samples per positive sample

    Returns:
        List of training dicts ready for MapleTrainer
    """
    samples = oracle["samples"]
    training_data = []

    logger.info(f"Building training data ({len(samples)} oracle samples)...")

    for idx, sample in enumerate(samples):
        question = sample["question"]
        top_ids = sample["top_5_block_ids"]
        all_block_texts = sample["all_block_texts"]

        # all_block_texts is a dict: {"0": "text", "1": "text", ...}
        block_ids = list(all_block_texts.keys())
        block_texts = list(all_block_texts.values())

        if len(block_ids) < 2:
            continue

        # Encode query
        query_emb = indexer.encode_query(question).cpu()

        # Encode all blocks for this sample at once
        block_embs = indexer.model.encode(
            block_texts,
            convert_to_tensor=True,
            show_progress_bar=False,
        ).cpu()

        # Create block_id -> embedding mapping
        id_to_emb = {bid: block_embs[i] for i, bid in enumerate(block_ids)}

        # Positive samples
        top_ids_str = [str(tid) for tid in top_ids]
        for tid in top_ids_str:
            if tid in id_to_emb:
                training_data.append({
                    "query_emb": query_emb,
                    "block_emb": id_to_emb[tid],
                    "label": 1,
                    "question": question,
                    "block_id": int(tid),
                })

        # Negative samples
        negative_ids = [bid for bid in block_ids if bid not in top_ids_str]
        num_neg = min(len(negative_ids), len(top_ids_str) * neg_ratio)
        neg_sample = random.sample(negative_ids, num_neg) if negative_ids else []

        for nid in neg_sample:
            training_data.append({
                "query_emb": query_emb,
                "block_emb": id_to_emb[nid],
                "label": 0,
                "question": question,
                "block_id": int(nid),
            })

        if (idx + 1) % 50 == 0:
            pos = sum(1 for d in training_data if d["label"] == 1)
            neg = len(training_data) - pos
            logger.info(
                f"  [{idx+1:>4d}/{len(samples)}] "
                f"total={len(training_data)} (pos={pos}, neg={neg})"
            )

    pos = sum(1 for d in training_data if d["label"] == 1)
    neg = len(training_data) - pos
    logger.info(f"Training data ready: {len(training_data)} samples (pos={pos}, neg={neg})")

    return training_data


# ===================================================================
# Plotting
# ===================================================================

def plot_learning_curve(history: dict, output_path: Path):
    """Plot loss and recall vs. epoch."""
    output_path.parent.mkdir(parents=True, exist_ok=True)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    epochs = history["epoch"]

    # Loss curve
    ax1.plot(epochs, history["loss"], "o-", color="#4c72b0", linewidth=2, markersize=4)
    ax1.set_xlabel("Epoch", fontsize=12)
    ax1.set_ylabel("Loss", fontsize=12)
    ax1.set_title("Training Loss", fontsize=14, fontweight="bold")
    ax1.grid(True, alpha=0.3)

    # Recall curve
    recall_pct = [r * 100 for r in history["val_recall"]]
    ax2.plot(epochs, recall_pct, "o-", color="#55a868", linewidth=2, markersize=4)
    ax2.set_xlabel("Epoch", fontsize=12)
    ax2.set_ylabel("Recall@5 (%)", fontsize=12)
    ax2.set_title("Validation Recall@5", fontsize=14, fontweight="bold")
    ax2.grid(True, alpha=0.3)

    best_epoch = epochs[recall_pct.index(max(recall_pct))]
    ax2.axvline(best_epoch, color="red", linestyle="--", alpha=0.5, label=f"Best (epoch {best_epoch})")
    ax2.legend()

    fig.suptitle("MAPLE Training Curves", fontsize=16, fontweight="bold", y=1.02)
    fig.tight_layout()
    fig.savefig(output_path, dpi=150, bbox_inches="tight")
    plt.close(fig)
    logger.info(f"Learning curve saved -> {output_path}")


# ===================================================================
# Main
# ===================================================================

def run(
    epochs: int = 20,
    batch_size: int = 32,
    lr: float = 1e-4,
    val_split: float = 0.2,
    device: str = "cuda",
):
    """Run the full training pipeline."""
    print("=" * 70)
    print("MAPLE Model Trainer")
    print(f"  Oracle Data:  {ORACLE_PATH}")
    print(f"  Output Model: {MODEL_PATH}")
    print(f"  Epochs:       {epochs}")
    print(f"  Batch Size:   {batch_size}")
    print(f"  LR:           {lr}")
    print(f"  Device:       {device}")
    print("=" * 70)

    # ---- Check prerequisites ----
    if not ORACLE_PATH.exists():
        logger.error(
            f"Oracle data not found at {ORACLE_PATH}. "
            "Run 'python scripts/generate_oracle.py' first."
        )
        sys.exit(1)

    # ---- Load oracle data ----
    oracle = load_oracle(ORACLE_PATH)

    # ---- Build training data with BGE embeddings ----
    indexer = MapleIndexer(device=device)    # this will lazy-load BGE
    training_data = build_training_data(oracle, indexer)

    if len(training_data) < 10:
        logger.error("Not enough training data. Need at least 10 samples.")
        sys.exit(1)

    # Free indexer GPU memory before training
    del indexer
    torch.cuda.empty_cache()

    # ---- Train ----
    start = time.time()
    trainer = MapleTrainer(
        input_dim=768,     # 384 (query) + 384 (block)
        hidden_dim=128,
        dropout=0.3,
        device=device,
    )

    MODEL_DIR.mkdir(parents=True, exist_ok=True)

    model, best_recall = trainer.train(
        training_data=training_data,
        epochs=epochs,
        batch_size=batch_size,
        lr=lr,
        val_split=val_split,
        save_path=str(MODEL_PATH),
    )

    elapsed = time.time() - start

    # ---- Plot learning curve ----
    plot_learning_curve(trainer.history_, CURVE_PATH)

    # ---- Summary ----
    print("\n" + "=" * 70)
    print("Training Complete!")
    print(f"  Best Recall@5: {best_recall*100:.1f}%")
    print(f"  Time:          {elapsed:.0f}s ({elapsed/60:.1f} min)")
    print(f"  Model:         {MODEL_PATH}")
    print(f"  Curve:         {CURVE_PATH}")
    print(f"  Parameters:    {model.num_parameters:,}")
    print("=" * 70)

    # Verify model is loadable
    from maplecore import MapleNet
    loaded = MapleNet.load(str(MODEL_PATH), device=device)
    logger.info(f"Model verified: {loaded.num_parameters:,} params")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MAPLE Model Trainer")
    parser.add_argument("--epochs", type=int, default=20)
    parser.add_argument("--batch-size", type=int, default=32)
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--val-split", type=float, default=0.2)
    parser.add_argument("--device", type=str, default="cuda")
    args = parser.parse_args()

    run(
        epochs=args.epochs,
        batch_size=args.batch_size,
        lr=args.lr,
        val_split=args.val_split,
        device=args.device,
    )
