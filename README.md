# Scout-KV ğŸ”­

**Speculative Paging for Infinite Context LLMs**

> Handle million-token documents on consumer hardware using learned attention patterns.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ¯ The Problem

Modern LLMs have limited context windows (4K-128K tokens). Current solutions have critical flaws:

| Approach | Problem |
|----------|---------|
| **Cloud APIs** | $$$, latency, privacy concerns |
| **Standard RAG** | Only **30% Recall** - misses relevant context |
| **Naive Chunking** | Destroys semantic boundaries |

## ğŸš€ The Solution: Scout-KV

Scout-KV learns which context blocks actually matter by analyzing LLM attention patterns:

```
Document (1M tokens) â†’ Scout (1ms) â†’ Top-5 Blocks â†’ LLM (50ms) â†’ Answer
```

### Key Results

| Metric | RAG | Scout-KV | Improvement |
|--------|-----|----------|-------------|
| **Recall@5** | 29.6% | **71.6%** | 2.4x better |
| **Latency** | 0.85ms | 2.71ms | Acceptable |
| **Model Size** | N/A | 100 KB | Tiny |

### Why It Works: 97% Attention Sparsity

We discovered that LLM attention is **extremely sparse**:
- Only **3%** of context blocks receive meaningful attention
- The rest can be safely pruned without affecting answer quality
- Scout learns to predict which blocks will be attended to

---

## ğŸ“¦ Installation

```bash
pip install scoutkv
```

Or from source:

```bash
git clone https://github.com/your-org/scout-kv.git
cd scout-kv
pip install -e .
```

---

## âš¡ Quick Start

```python
from scoutkv import ScoutKV

# Initialize client
client = ScoutKV(model_path="scout_bge.pth")

# Index a document
client.index_file("books/sherlock_holmes.txt")

# Query
results = client.query("What was the speckled band?")

# Get relevant context
context = client.get_context(results, max_blocks=5)
print(context)
```

### Search Strategies

```python
# Adaptive (default): Entropy-aware with dynamic K
results = client.query("Who is the killer?", strategy="adaptive")

# Hierarchical: For 50K+ blocks
results = client.query("Find the murder weapon", strategy="hierarchical")

# Linear: Simple top-k
results = client.query("Describe Watson", strategy="linear")
```

---

## ğŸ—ï¸ Architecture

```
scoutkv/
â”œâ”€â”€ core.py       # ScoutBGE model (768 â†’ 128 â†’ 1)
â”œâ”€â”€ indexer.py    # BGE embedding, chunking, I/O
â”œâ”€â”€ search.py     # Linear, Hierarchical, Adaptive search
â”œâ”€â”€ trainer.py    # Training logic
â””â”€â”€ utils.py      # Device handling, helpers
```

### Components

| Component | Description |
|-----------|-------------|
| **Indexer** | Chunks documents and generates BGE embeddings |
| **ScoutBGE** | Lightweight MLP that scores block relevance |
| **Scanner** | Implements search strategies |
| **ScoutKV** | High-level client API |

---

## ğŸ§ª Training Your Own Scout

```python
from scoutkv import ScoutTrainer, Indexer

# Prepare training data with oracle labels
training_data = [
    {"query_emb": q, "block_emb": b, "label": 1, "question": "...", "block_id": 0},
    ...
]

# Train
trainer = ScoutTrainer(device="cuda")
model, recall = trainer.train(
    training_data,
    epochs=20,
    save_path="my_scout.pth"
)

print(f"Best Recall@5: {recall*100:.1f}%")
```

---

## ğŸ“Š Benchmarks

### Scale Performance (Hierarchical Search)

| Blocks | Linear | Hierarchical | Speedup |
|--------|--------|--------------|---------|
| 1,000 | 18ms | 1.2ms | 15x |
| 50,000 | 900ms | 60ms | 15x |
| 100,000 | 1.8s | 120ms | 15x |

### Quantization (INT8)

| Metric | FP32 | INT8 | Delta |
|--------|------|------|-------|
| Size | 378 KB | 98 KB | -74% |
| Accuracy | 71.6% | 71.6% | 0% |

---

## ğŸ”§ Configuration

```python
from scoutkv import ScoutKV

client = ScoutKV(
    model_path="scout_bge.pth",
    device="cuda",           # or "cpu"
    chunk_size=500,          # characters per block
    strategy="adaptive"      # default search strategy
)
```

### Adaptive Search Parameters

The Scanner class accepts tuning parameters:

```python
from scoutkv import Scanner, ScoutBGE

model = ScoutBGE.load("scout_bge.pth")
scanner = Scanner(
    model,
    confidence_threshold=0.15,  # Below this â†’ RAG fallback
    mass_target=0.80,           # Accumulate until 80% attention mass
    max_blocks=20,              # Never return more than this
    chapter_size=100            # Blocks per chapter (hierarchical)
)
```

---

## ğŸ“š Examples

See the `examples/` directory:

- `demo_infinite_context.py` - Basic usage with Sherlock Holmes
- `benchmark_vs_rag.py` - Compare Scout-KV vs standard RAG

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `pytest tests/`
5. Submit a pull request

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) for embeddings
- [Meta Llama 3](https://llama.meta.com/) for attention oracle generation
- [NarrativeQA](https://github.com/deepmind/narrativeqa) dataset

---

**Built with â¤ï¸ for the infinite context future.**
